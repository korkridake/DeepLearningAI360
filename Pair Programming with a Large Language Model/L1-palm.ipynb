{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e80b742-0c2e-46d2-99e5-61f2982571e4",
   "metadata": {},
   "source": [
    "# Lesson 1: Getting Started with PaLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b4dd3-3436-43f8-811c-262ed83d7767",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "Set the ~~MakerSuite~~ Gemini API key with the provided helper function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a7b619",
   "metadata": {},
   "source": [
    "> Note: this course was launched in 2023, up to date (October 2024) [PaLM is being deprecated](https://ai.google.dev/palm_docs/deprecation). Therefore, we're migrating the content of this notebook [from PaLM to Gemini](https://ai.google.dev/docs/migration_guide) to be functional from now on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2db7275e-7ba3-482c-90a5-8d470dcca05c",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from utils import get_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8be2f53-efa5-495f-808e-1e3189f0b73d",
   "metadata": {},
   "source": [
    "In this classroom, we've installed the relevant libraries for you.\n",
    "\n",
    "If you wanted to use the ~~PaLM API~~ Gemini API on your own machine, you would first install the library:\n",
    "```Python\n",
    "!pip install -q google.generativeai\n",
    "```\n",
    "The optional flag `-q` installs \"quietly\" without printing out details of the installation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71786bc2",
   "metadata": {},
   "source": [
    "> Note: if you want to run it locally, you can get the Gemini API from this [website](https://aistudio.google.com/app/apikey) by only using your gmail account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ffb615",
   "metadata": {},
   "source": [
    "```Python\n",
    "# Legacy PALM API code shown in the video\n",
    "import google.generativeai as palm\n",
    "palm.configure(api_key=get_api_key())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89a9a4b3-b338-4ed8-ac7b-a08143da5b63",
   "metadata": {
    "height": 217
   },
   "outputs": [],
   "source": [
    "# From now own with Gemini API\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from google.api_core import client_options as client_options_lib\n",
    "\n",
    "genai.configure(\n",
    "    api_key=get_api_key(),\n",
    "    transport=\"rest\",\n",
    "    client_options=client_options_lib.ClientOptions(\n",
    "        api_endpoint=os.getenv(\"GOOGLE_API_BASE\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9648b897-5ad4-4caa-808d-97528c2fcf39",
   "metadata": {},
   "source": [
    "### Explore the available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77038a39-427c-4d1f-bc7e-e0692e8f6869",
   "metadata": {
    "height": 149
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: models/chat-bison-001\n",
      "description: A legacy text-only model optimized for chat conversations\n",
      "generation methods:['generateMessage', 'countMessageTokens']\n",
      "\n",
      "name: models/text-bison-001\n",
      "description: A legacy model that understands text and generates text as an output\n",
      "generation methods:['generateText', 'countTextTokens', 'createTunedTextModel']\n",
      "\n",
      "name: models/embedding-gecko-001\n",
      "description: Obtain a distributed representation of a text.\n",
      "generation methods:['embedText', 'countTextTokens']\n",
      "\n",
      "name: models/gemini-1.0-pro-vision-latest\n",
      "description: The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.\n",
      "generation methods:['generateContent', 'countTokens']\n",
      "\n",
      "name: models/gemini-pro-vision\n",
      "description: The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.\n",
      "generation methods:['generateContent', 'countTokens']\n",
      "\n",
      "name: models/gemini-1.5-pro-latest\n",
      "description: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.\n",
      "generation methods:['generateContent', 'countTokens']\n",
      "\n",
      "name: models/gemini-1.5-pro-001\n",
      "description: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.\n",
      "generation methods:['generateContent', 'countTokens', 'createCachedContent']\n",
      "\n",
      "name: models/gemini-1.5-pro-002\n",
      "description: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in September of 2024.\n",
      "generation methods:['generateContent', 'countTokens', 'createCachedContent']\n",
      "\n",
      "name: models/gemini-1.5-pro\n",
      "description: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.\n",
      "generation methods:['generateContent', 'countTokens']\n",
      "\n",
      "name: models/gemini-1.5-flash-latest\n",
      "description: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.\n",
      "generation methods:['generateContent', 'countTokens']\n",
      "\n",
      "name: models/gemini-1.5-flash-001\n",
      "description: Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.\n",
      "generation methods:['generateContent', 'countTokens', 'createCachedContent']\n",
      "\n",
      "name: models/gemini-1.5-flash-001-tuning\n",
      "description: Version of Gemini 1.5 Flash that supports tuning, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.\n",
      "generation methods:['generateContent', 'countTokens', 'createTunedModel']\n",
      "\n",
      "name: models/gemini-1.5-flash\n",
      "description: Alias that points to the most recent stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.\n",
      "generation methods:['generateContent', 'countTokens']\n",
      "\n",
      "name: models/gemini-1.5-flash-002\n",
      "description: Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in September of 2024.\n",
      "generation methods:['generateContent', 'countTokens', 'createCachedContent']\n",
      "\n",
      "name: models/gemini-1.5-flash-8b\n",
      "description: Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n",
      "generation methods:['createCachedContent', 'generateContent', 'countTokens']\n",
      "\n",
      "name: models/gemini-1.5-flash-8b-001\n",
      "description: Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n",
      "generation methods:['createCachedContent', 'generateContent', 'countTokens']\n",
      "\n",
      "name: models/gemini-1.5-flash-8b-latest\n",
      "description: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n",
      "generation methods:['createCachedContent', 'generateContent', 'countTokens']\n",
      "\n",
      "name: models/gemini-1.5-flash-8b-exp-0827\n",
      "description: Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).\n",
      "generation methods:['generateContent', 'countTokens']\n",
      "\n",
      "name: models/gemini-1.5-flash-8b-exp-0924\n",
      "description: Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).\n",
      "generation methods:['generateContent', 'countTokens']\n",
      "\n",
      "name: models/gemini-2.5-pro-exp-03-25\n",
      "description: Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n",
      "generation methods:['generateContent', 'countTokens']\n",
      "\n",
      "name: models/gemini-2.5-pro-preview-03-25\n",
      "description: Gemini 2.5 Pro Preview 03-25\n",
      "generation methods:['generateContent', 'countTokens']\n",
      "\n",
      "name: models/gemini-2.0-flash-exp\n",
      "description: Gemini 2.0 Flash Experimental\n",
      "generation methods:['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "\n",
      "name: models/gemini-2.0-flash\n",
      "description: Gemini 2.0 Flash\n",
      "generation methods:['generateContent', 'countTokens']\n",
      "\n",
      "name: models/gemini-2.0-flash-001\n",
      "description: Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025.\n",
      "generation methods:['generateContent', 'countTokens']\n",
      "\n",
      "name: models/gemini-2.0-flash-exp-image-generation\n",
      "description: Gemini 2.0 Flash (Image Generation) Experimental\n",
      "generation methods:['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "\n",
      "name: models/gemini-2.0-flash-lite-001\n",
      "description: Stable version of Gemini 2.0 Flash Lite\n",
      "generation methods:['generateContent', 'countTokens']\n",
      "\n",
      "name: models/gemini-2.0-flash-lite\n",
      "description: Gemini 2.0 Flash-Lite\n",
      "generation methods:['generateContent', 'countTokens']\n",
      "\n",
      "name: models/gemini-2.0-flash-lite-preview-02-05\n",
      "description: Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite\n",
      "generation methods:['generateContent', 'countTokens']\n",
      "\n",
      "name: models/gemini-2.0-flash-lite-preview\n",
      "description: Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite\n",
      "generation methods:['generateContent', 'countTokens']\n",
      "\n",
      "name: models/gemini-2.0-pro-exp\n",
      "description: Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n",
      "generation methods:['generateContent', 'countTokens']\n",
      "\n",
      "name: models/gemini-2.0-pro-exp-02-05\n",
      "description: Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n",
      "generation methods:['generateContent', 'countTokens']\n",
      "\n",
      "name: models/gemini-exp-1206\n",
      "description: Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n",
      "generation methods:['generateContent', 'countTokens']\n",
      "\n",
      "name: models/gemini-2.0-flash-thinking-exp-01-21\n",
      "description: Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking\n",
      "generation methods:['generateContent', 'countTokens']\n",
      "\n",
      "name: models/gemini-2.0-flash-thinking-exp\n",
      "description: Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking\n",
      "generation methods:['generateContent', 'countTokens']\n",
      "\n",
      "name: models/gemini-2.0-flash-thinking-exp-1219\n",
      "description: Gemini 2.0 Flash Thinking Experimental\n",
      "generation methods:['generateContent', 'countTokens']\n",
      "\n",
      "name: models/learnlm-1.5-pro-experimental\n",
      "description: Alias that points to the most recent stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.\n",
      "generation methods:['generateContent', 'countTokens']\n",
      "\n",
      "name: models/gemma-3-1b-it\n",
      "description: \n",
      "generation methods:['generateContent', 'countTokens']\n",
      "\n",
      "name: models/gemma-3-4b-it\n",
      "description: \n",
      "generation methods:['generateContent', 'countTokens']\n",
      "\n",
      "name: models/gemma-3-12b-it\n",
      "description: \n",
      "generation methods:['generateContent', 'countTokens']\n",
      "\n",
      "name: models/gemma-3-27b-it\n",
      "description: \n",
      "generation methods:['generateContent', 'countTokens']\n",
      "\n",
      "name: models/embedding-001\n",
      "description: Obtain a distributed representation of a text.\n",
      "generation methods:['embedContent']\n",
      "\n",
      "name: models/text-embedding-004\n",
      "description: Obtain a distributed representation of a text.\n",
      "generation methods:['embedContent']\n",
      "\n",
      "name: models/gemini-embedding-exp-03-07\n",
      "description: Obtain a distributed representation of a text.\n",
      "generation methods:['embedContent']\n",
      "\n",
      "name: models/gemini-embedding-exp\n",
      "description: Obtain a distributed representation of a text.\n",
      "generation methods:['embedContent']\n",
      "\n",
      "name: models/aqa\n",
      "description: Model trained to return answers to questions that are grounded in provided sources, along with estimating answerable probability.\n",
      "generation methods:['generateAnswer']\n",
      "\n",
      "name: models/imagen-3.0-generate-002\n",
      "description: Vertex served Imagen 3.0 002 model\n",
      "generation methods:['predict']\n",
      "\n",
      "name: models/veo-2.0-generate-001\n",
      "description: Vertex served Veo 2 model.\n",
      "generation methods:['predictLongRunning']\n",
      "\n",
      "name: models/gemini-2.0-flash-live-001\n",
      "description: Gemini 2.0 Flash 001\n",
      "generation methods:['bidiGenerateContent', 'countTokens']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PaLM API legacy:\n",
    "# palm.list_models()\n",
    "\n",
    "# Now, Gemini API\n",
    "for m in genai.list_models():\n",
    "    print(f\"name: {m.name}\")\n",
    "    print(f\"description: {m.description}\")\n",
    "    print(f\"generation methods:{m.supported_generation_methods}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8540099-fad0-4954-83a7-c2fba3f6d972",
   "metadata": {},
   "source": [
    "#### Filter models by their supported generation methods\n",
    "- `generateText` is currently recommended for coding-related prompts.\n",
    "- `generateMessage` is optimized for multi-turn chats (dialogues) with an LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1e8e83",
   "metadata": {},
   "source": [
    "> Update (October 2024):\n",
    "- `generateContent`, best model for scaling across a wide range of tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48e26d6a-02b9-4838-a0e6-d2e6a3ae042e",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(name='models/text-bison-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='PaLM 2 (Legacy)',\n",
       "       description='A legacy model that understands text and generates text as an output',\n",
       "       input_token_limit=8196,\n",
       "       output_token_limit=1024,\n",
       "       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
       "       temperature=0.7,\n",
       "       top_p=0.95,\n",
       "       top_k=40)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [m for m in genai.list_models() \n",
    "          if 'generateText' \n",
    "          in m.supported_generation_methods]\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eb4fc7d-2a1a-43bc-9810-25e4db3b7cb7",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(name='models/text-bison-001',\n",
       "      base_model_id='',\n",
       "      version='001',\n",
       "      display_name='PaLM 2 (Legacy)',\n",
       "      description='A legacy model that understands text and generates text as an output',\n",
       "      input_token_limit=8196,\n",
       "      output_token_limit=1024,\n",
       "      supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
       "      temperature=0.7,\n",
       "      top_p=0.95,\n",
       "      top_k=40)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Bison in 2024 is a legacy model  \n",
    "model_bison = models[0]\n",
    "model_bison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a24bff-ebe0-4fd3-93f6-c2aeef4d44f7",
   "metadata": {},
   "source": [
    "#### helper function to generate text\n",
    "\n",
    "- The `@retry` decorator helps you to retry the API call if it fails.\n",
    "- We set the temperature to 0.0 so that the model returns the same output (completion) if given the same input (the prompt).\n",
    "\n",
    "```Python\n",
    "# Code legacy for PALM API\n",
    "from google.api_core import retry\n",
    "@retry.Retry()\n",
    "def generate_text(prompt,\n",
    "                  model=model_bison,\n",
    "                  temperature=0.0):\n",
    "    return palm.generate_text(prompt=prompt,\n",
    "                              model=model,\n",
    "                              temperature=temperature)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f2fd6e9",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Set the model to connect to the Gemini API\n",
    "model_flash = genai.GenerativeModel(model_name='gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a73e1ce1",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "# Helper with Gemini API\n",
    "def generate_text(prompt,\n",
    "                  model=model_flash,\n",
    "                  temperature=0.0):\n",
    "    return model_flash.generate_content(prompt,\n",
    "                                  generation_config={'temperature':temperature})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa756beb-7e70-4575-a27e-82b733b3d3b0",
   "metadata": {},
   "source": [
    "#### Ask the LLM how to write some code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04080420-acd1-43a8-92bc-7d4c407a0154",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "prompt = \"Show me how to iterate across a list in Python.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32a354db-cb9b-4353-b777-4980256f4686",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Gemini API updates to generate the text\n",
    "completion = generate_text(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef2b1688-2eb7-465c-81cd-555d5b0a5a70",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python offers several ways to iterate across a list. Here are the most common methods, with explanations and examples:\n",
      "\n",
      "**1. Using a `for` loop:** This is the most straightforward and commonly used method.\n",
      "\n",
      "```python\n",
      "my_list = [\"apple\", \"banana\", \"cherry\"]\n",
      "\n",
      "# Iterate and print each item\n",
      "for item in my_list:\n",
      "    print(item)\n",
      "\n",
      "# Iterate and print the index and value\n",
      "for i, item in enumerate(my_list):\n",
      "    print(f\"Item at index {i}: {item}\")\n",
      "```\n",
      "\n",
      "**2. Using a `while` loop:** This gives you more control, especially if you need to manipulate the list's indices during iteration.\n",
      "\n",
      "```python\n",
      "my_list = [\"apple\", \"banana\", \"cherry\"]\n",
      "i = 0\n",
      "while i < len(my_list):\n",
      "    print(my_list[i])\n",
      "    i += 1\n",
      "```\n",
      "\n",
      "**3. List comprehension:**  While not strictly iteration in the same sense as `for` and `while` loops, list comprehensions provide a concise way to create new lists based on existing ones.  They implicitly iterate.\n",
      "\n",
      "```python\n",
      "my_list = [\"apple\", \"banana\", \"cherry\"]\n",
      "\n",
      "# Create a new list with uppercase strings\n",
      "uppercase_list = [item.upper() for item in my_list]\n",
      "print(uppercase_list)\n",
      "\n",
      "# Create a new list containing only fruits with more than 5 letters\n",
      "long_fruits = [item for item in my_list if len(item) > 5]\n",
      "print(long_fruits)\n",
      "```\n",
      "\n",
      "**4. Using `iter()` and `next()`:** This approach allows for more manual control over the iteration process.  It's less common for simple list iteration but useful in more advanced scenarios.\n",
      "\n",
      "```python\n",
      "my_list = [\"apple\", \"banana\", \"cherry\"]\n",
      "my_iterator = iter(my_list)\n",
      "\n",
      "try:\n",
      "    while True:\n",
      "        item = next(my_iterator)\n",
      "        print(item)\n",
      "except StopIteration:\n",
      "    pass # This exception is raised when the iterator is exhausted\n",
      "```\n",
      "\n",
      "\n",
      "**Which method should you use?**\n",
      "\n",
      "* For simple iteration and printing or processing each item, the `for` loop is the most readable and efficient.  Using `enumerate` within the `for` loop is best if you need both the index and the value.\n",
      "* Use a `while` loop when you need more control over the iteration process, such as conditionally skipping items or modifying the list during iteration.\n",
      "* List comprehensions are ideal for creating new lists based on existing ones in a concise way.\n",
      "* `iter()` and `next()` are generally used for more advanced scenarios where you need fine-grained control over the iteration, such as custom iterators or working with generators.\n",
      "\n",
      "\n",
      "Remember to choose the method that best suits your needs and coding style, prioritizing readability and maintainability.  For most everyday list iteration tasks, the simple `for` loop is the best choice.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PaLM API\n",
    "## print(completion.result)\n",
    "\n",
    "# Gemini API\n",
    "print(completion.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1400bcb5-bfe8-4192-809d-d95b21bf8422",
   "metadata": {},
   "source": [
    "- **Tip:** The words \"show me\" tends to encourage the ~~PaLM~~ Gemini LLM to give more details and explanations compared to if you were to ask \"write code to ...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b813473-15de-4672-9097-57a3d04219d6",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "prompt = \"write code to iterate across a list in Python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3557557d-2b86-4755-a44f-8846e0035d3a",
   "metadata": {
    "height": 132
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are several ways to iterate across a list in Python, each with its own advantages:\n",
      "\n",
      "**1. Using a `for` loop:** This is the most common and often the most readable approach.\n",
      "\n",
      "```python\n",
      "my_list = [10, 20, 30, 40, 50]\n",
      "\n",
      "# Iterate and print each element\n",
      "for item in my_list:\n",
      "    print(item)\n",
      "\n",
      "# Iterate and print the index and element\n",
      "for i, item in enumerate(my_list):\n",
      "    print(f\"Index: {i}, Value: {item}\")\n",
      "```\n",
      "\n",
      "**2. Using a `while` loop:** This gives you more control over the iteration process, allowing you to manipulate the index directly.  However, it's generally less readable than a `for` loop for simple list iteration.\n",
      "\n",
      "```python\n",
      "my_list = [10, 20, 30, 40, 50]\n",
      "i = 0\n",
      "while i < len(my_list):\n",
      "    print(my_list[i])\n",
      "    i += 1\n",
      "```\n",
      "\n",
      "**3. List comprehension:**  While not strictly iteration in the same sense as `for` and `while` loops, list comprehensions provide a concise way to create a new list based on an existing one.\n",
      "\n",
      "```python\n",
      "my_list = [10, 20, 30, 40, 50]\n",
      "\n",
      "# Create a new list with each element doubled\n",
      "doubled_list = [item * 2 for item in my_list]\n",
      "print(doubled_list)\n",
      "\n",
      "# Create a new list containing only even numbers\n",
      "even_numbers = [item for item in my_list if item % 2 == 0]\n",
      "print(even_numbers)\n",
      "```\n",
      "\n",
      "**4. Using `iter()` and `next()`:** This approach is useful when you need more fine-grained control over the iteration, such as stopping early based on a condition.\n",
      "\n",
      "```python\n",
      "my_list = [10, 20, 30, 40, 50]\n",
      "my_iterator = iter(my_list)\n",
      "\n",
      "try:\n",
      "    while True:\n",
      "        item = next(my_iterator)\n",
      "        print(item)\n",
      "        if item == 30:  # Stop iteration when 30 is encountered\n",
      "            break\n",
      "except StopIteration:\n",
      "    pass\n",
      "```\n",
      "\n",
      "\n",
      "The best method depends on your specific needs.  For simple iteration, the `for` loop is generally preferred for its readability and ease of use.  For more complex scenarios or when you need fine-grained control, `while` loops, list comprehensions, or `iter()`/`next()` might be more appropriate.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PaLM API\n",
    "## completion = generate_text(prompt)\n",
    "## print(completion.result)\n",
    "\n",
    "# Gemini API\n",
    "completion = generate_text(prompt)\n",
    "print(completion.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26114873-bb3c-4253-a679-4dda28af561c",
   "metadata": {},
   "source": [
    "#### Try out the code\n",
    "- Try copy-pasting some of the generated code and running it in the notebook.\n",
    "- Remember to test out the LLM-generated code and debug it make sure it works as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2e76677-1b90-4ce4-a3b4-aae857e870f6",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are several ways to iterate across a list in Python. Here are a few examples, each with slightly different characteristics:\n",
      "\n",
      "**1. Using a `for` loop (most common and recommended):**\n",
      "\n",
      "This is the simplest and most readable way to iterate through a list.\n",
      "\n",
      "```python\n",
      "my_list = [10, 20, 30, 40, 50]\n",
      "\n",
      "for item in my_list:\n",
      "    print(item)\n",
      "```\n",
      "\n",
      "This loop iterates through each element in `my_list` and assigns it to the variable `item` in each iteration.\n",
      "\n",
      "\n",
      "**2. Using a `for` loop with index:**\n",
      "\n",
      "If you need the index of each element as well, you can use the `enumerate()` function:\n",
      "\n",
      "```python\n",
      "my_list = [10, 20, 30, 40, 50]\n",
      "\n",
      "for index, item in enumerate(my_list):\n",
      "    print(f\"Item at index {index}: {item}\")\n",
      "```\n",
      "\n",
      "`enumerate()` returns pairs of (index, item) for each element in the list.\n",
      "\n",
      "\n",
      "**3. Using a `while` loop:**\n",
      "\n",
      "While less common for iterating lists, a `while` loop can also be used.  This requires manual index management.\n",
      "\n",
      "```python\n",
      "my_list = [10, 20, 30, 40, 50]\n",
      "i = 0\n",
      "while i < len(my_list):\n",
      "    print(my_list[i])\n",
      "    i += 1\n",
      "```\n",
      "\n",
      "This approach is generally less preferred for list iteration because it's more verbose and prone to errors (e.g., forgetting to increment `i`).\n",
      "\n",
      "\n",
      "**4. List comprehension (for creating new lists):**\n",
      "\n",
      "List comprehensions are concise ways to create new lists based on existing ones.  While not strictly iteration in the same sense as the above examples, it processes each element:\n",
      "\n",
      "```python\n",
      "my_list = [10, 20, 30, 40, 50]\n",
      "\n",
      "# Double each element\n",
      "doubled_list = [item * 2 for item in my_list]\n",
      "print(doubled_list)  # Output: [20, 40, 60, 80, 100]\n",
      "\n",
      "# Only even numbers\n",
      "even_numbers = [item for item in my_list if item % 2 == 0]\n",
      "print(even_numbers) # Output: [10, 20, 40, 50]\n",
      "```\n",
      "\n",
      "List comprehensions are efficient and elegant for transforming lists.\n",
      "\n",
      "\n",
      "The `for` loop (method 1) is generally the most Pythonic and readable way to iterate through a list unless you specifically need the index or are creating a new list using a transformation.  Choose the method that best suits your needs and coding style, prioritizing readability and maintainability.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Paste the LLM's code here\n",
    "prompt = \"Write code to iterate across a list in Python\"\n",
    "\n",
    "# Gemini API\n",
    "completion = generate_text(prompt)\n",
    "print(completion.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c649daa-781c-4c69-ac1b-d100e9747190",
   "metadata": {},
   "source": [
    "#### Try asking your own coding question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c69b4929-ec4f-495c-a773-a92ce2c9b36c",
   "metadata": {
    "height": 149
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotly offers several ways to create bar plots, depending on the structure of your data. Here are examples using the `plotly.graph_objects` and `plotly.express` modules, which are the two primary ways to interact with Plotly in Python.\n",
      "\n",
      "**Method 1: Using `plotly.graph_objects` (for more control)**\n",
      "\n",
      "This method gives you fine-grained control over every aspect of the plot.  It's best when you need precise customization or are working with complex data structures.\n",
      "\n",
      "```python\n",
      "import plotly.graph_objects as go\n",
      "\n",
      "# Sample data\n",
      "categories = ['A', 'B', 'C', 'D']\n",
      "values = [20, 14, 24, 11]\n",
      "\n",
      "fig = go.Figure(data=[go.Bar(x=categories, y=values)])\n",
      "\n",
      "# Customize the plot\n",
      "fig.update_layout(\n",
      "    title='Simple Bar Plot',\n",
      "    xaxis_title='Categories',\n",
      "    yaxis_title='Values',\n",
      "    plot_bgcolor='#f2f2f2',  # Light gray background\n",
      "    paper_bgcolor='#f2f2f2'   # Light gray paper background\n",
      ")\n",
      "\n",
      "fig.show()\n",
      "```\n",
      "\n",
      "This code creates a simple bar plot.  You can customize many aspects, such as:\n",
      "\n",
      "* **`x` and `y`:**  Specify the x and y data.\n",
      "* **`marker`:** Control the appearance of the bars (color, line width, etc.).  Example: `marker=dict(color='blue', line=dict(color='black', width=1.5))`\n",
      "* **`text`:** Add text labels to each bar (e.g., values). Example: `go.Bar(x=categories, y=values, text=values, textposition='auto')`\n",
      "* **`hovertemplate`:** Customize the information displayed on hover.\n",
      "* **`update_layout`:** Modify the title, axis labels, background color, etc.\n",
      "\n",
      "\n",
      "**Method 2: Using `plotly.express` (for concise code)**\n",
      "\n",
      "`plotly.express` provides a higher-level interface, making it easier to create plots quickly.  It's ideal for simpler plots and when you don't need extensive customization.\n",
      "\n",
      "```python\n",
      "import plotly.express as px\n",
      "\n",
      "# Sample data (can be a Pandas DataFrame or a dictionary)\n",
      "data = {'Category': ['A', 'B', 'C', 'D'], 'Value': [20, 14, 24, 11]}\n",
      "\n",
      "fig = px.bar(data, x='Category', y='Value', title='Simple Bar Plot with Plotly Express')\n",
      "\n",
      "fig.show()\n",
      "```\n",
      "\n",
      "This creates the same basic bar plot with much less code.  `plotly.express` automatically handles many aspects of the plot's appearance.  You can still customize it using arguments like:\n",
      "\n",
      "* **`color`:** Color bars based on a categorical variable.\n",
      "* **`barmode`:**  Control how bars are displayed (e.g., 'group', 'stack', 'overlay').\n",
      "* **`color_discrete_sequence`:** Specify a list of colors for the bars.\n",
      "* **`labels`:** Change axis labels.\n",
      "* **`title`:** Set the plot title.\n",
      "\n",
      "\n",
      "**Horizontal Bar Plot (both methods)**\n",
      "\n",
      "To create a horizontal bar plot, simply swap the `x` and `y` arguments:\n",
      "\n",
      "**`plotly.graph_objects`:**\n",
      "\n",
      "```python\n",
      "fig = go.Figure(data=[go.Bar(x=values, y=categories, orientation='h')])\n",
      "fig.show()\n",
      "```\n",
      "\n",
      "**`plotly.express`:**\n",
      "\n",
      "```python\n",
      "fig = px.bar(data, x='Value', y='Category', orientation='h', title='Horizontal Bar Plot')\n",
      "fig.show()\n",
      "```\n",
      "\n",
      "Remember to install Plotly:  `pip install plotly`\n",
      "\n",
      "Choose the method that best suits your needs.  `plotly.express` is generally quicker for simple plots, while `plotly.graph_objects` offers more control for complex visualizations.  Explore the Plotly documentation for more advanced customization options.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Modify the prompt with your own question\n",
    "prompt = \"show me how to create a bar plot using Plotly\"\n",
    "\n",
    "# Gemini API\n",
    "completion = generate_text(prompt)\n",
    "print(completion.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2b9e7a-4911-476d-9141-010224682d17",
   "metadata": {},
   "source": [
    "#### Note about the API key\n",
    "We've provided an API key for this classroom.  If you would like your own API key for your own projects, you can get one at [developers.generativeai.google](https://developers.generativeai.google/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
